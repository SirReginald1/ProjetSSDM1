---
title: "Raport"
date: last-modified
format: 
  html:
    fig-format: svg
    embed-resources: true
toc: true
toc-title: "Summary"
toc-location: left
toc-depth: 3
lightbox: true
---

```{r, echo=FALSE, warning=FALSE, message=FALSE}
knitr::opts_chunk$set(warning = FALSE, echo = FALSE, fig.pos="H", message = FALSE)
source("Scripts/functions.R")
if(!require("ggplot2")){install.packages("ggplot2"); library(ggplot2)}
if(!require("kableExtra")){install.packages("kableExtra"); library(kableExtra)}
if(!require("ggpubr")){install.packages("ggpubr"); library(ggpubr)}
if(!require("jsonlite")){install.packages("jsonlite"); library(jsonlite)}
if(!require("corrplot")){install.packages("corrplot"); library(corrplot)}
if(!require("e1071")){install.packages("e1071"); library(e1071)}
if(!require("xgboost")){install.packages("xgboost"); library(xgboost)}
```


# Préparation des données


```{r}
n = 10000
data_train = readRDS(paste0("Data/df_newas", n, "_train.rds"))
data_test = readRDS(paste0("Data/df_newas", n, "_test.rds"))
data_train$meth_class = factor(data_train$meth_class)
data_test$meth_class = factor(data_test$meth_class)
data_full = rbind(data_test, data_train)
origin_train = readRDS(paste0("OldData/df_newas", n, "_train.rds"))$meth_class
origin_test = readRDS(paste0("OldData/df_newas", n, "_test.rds"))$meth_class
```

Pour commencer nous nous sommes rendu compte lors de l'implémentation de certains algorithmes que la répartition des classes été déséquilibré entre le group train et test, comme on peut le voir dans la @fig-train_test_origin.

:::{#fig-train_test_origin}

```{r, fig.width=15, fig.height=7}
ggplot(data.frame("Class" = c(origin_train, origin_test),
                    "Dataset" = factor(c(rep("Train", length(origin_train)), rep("Test", length(origin_test))))),
         aes(Class))+
    geom_bar(aes(fill = Dataset), position="fill") +
    theme(axis.text.x = element_text(angle = 45, face = "bold", hjust = 1)) +
    labs(y = "Proportion of observations")
```

Répartition originale des classes entre les données train et test.

:::

Cela peut diminuer la performance des algorithmes de classification. De plus certaines classes se trouve exclusivement dans le groupe d'entraînement ce qui fausse les mesure d'accuracy.


Comme on peut le voir dans la @fig-disti_class_origin, 3 classes n'ont qu'une seule observation. Nous avons donc retiré ces observations pour les classes suivantes, car on ne peut pas les séparer dans les groupes entraînement et test:

- CONTR, CEBM

- CONTR, HEMI

- CONTR, INFLAM

:::{#fig-disti_class_origin}

```{r, fig.width=15, fig.height=7}
data_full = data.frame("Class" = c(origin_train, origin_test))
ggplot(data_full, aes(factor(Class, 
                        levels = names(rev(sort(table(Class))))
                        )
                 )
       ) + geom_bar(fill = "steelblue") + 
  theme(axis.text.x = element_text(angle = 45, face = "bold", hjust = 1)) +
  labs(y = "Count", x= "Class")
```

Nombres d'observation par classe.

:::

Pour pour adresser le problème de déséquilibre des classes, 2 jeux de données on été créé. 
Dans le premier, les groupes d'entraînement et de test original ont été gardés. Et seules les classes présentes à la fois dans les données d'entraînement et de test, on été gardé. Cela a donc réduit le nombre de classe à prédire à 69.

Dans le deuxième jeu de donnée, la répartition de chaque classe à prédire a été répartie équitablement entre les groupes d'entraînement et de test, comme indiquée dans la @fig-train_test_balanced.

Chaque gène a ensuite été classé par rapport à sa capacité à expliquer le type de cancer sur chacun des 2 group d'entraînement. Le classement a été réalisé grâce à la p-valeur de la statistique de student d'un modèle linéaire.

Le résultat se trouve dans la @fig-train_test_balanced.

<details>
<summary style="font-weight: bold; color: #72afd2;">Code rééquilibrage</summary>
```{r, echo = TRUE, eval = FALSE}
data_test = readRDS("Data/df_preproc_test.rds")
data_train = readRDS("Data/df_preproc_train.rds")
intersect = intersect(names(data_train)[c(1,64:ncol(data_train))],
                      names(data_test))
data_full = rbind(data_test[,intersect],
                  data_train[,intersect])
data_full = data_full[!(data_full$meth_class %in% names(sort(table(data_full$meth_class)))[1:3]),]
order = caret::createDataPartition(data_full$meth_class, p = .6, list = FALSE)
saveRDS(data_full[-order,], "Data/df_preproc_test.rds", compress = TRUE)
saveRDS(data_full[order,], "df_preproc_train.rds", compress = TRUE)
```
</details>

:::{#fig-train_test_balanced}

```{r, fig.width=15, fig.height=7}
plot_split(data_train, data_test)
```

Nouvel répartition des classes entre les données train et test.

:::

:::{#fig-disti_class_balanced}

```{r, fig.width=15, fig.height=7}
data_full = data.frame("Class" = c(data_train$meth_class, data_test$meth_class))
ggplot(data_full, aes(factor(Class, 
                        levels = names(rev(sort(table(Class))))
                        )
                 )
       ) + geom_bar(fill = "steelblue") + 
  theme(axis.text.x = element_text(angle = 45, face = "bold", hjust = 1)) +
  labs(y = "Count", x= "Class")
```

Nombres d'observation par classe des nouveaux jeux de donnée.

:::

Une fois les classe qui n'apparaisse pas dans le jeu de données test original, on été enlevé au jeu de donnée d'entraînement originale, on retrouve avec les jeux de donnée qui ont les caractéristiques suivantes:

Le nombre de classe à prédire et donc passer de `r length(unique(c(origin_test, origin_train)))` à `r length(unique(c(data_test$meth_class, data_train$meth_class)))` pour le jeu de donné équilibré et a `r length(unique(origin_test[origin_test %in% Original_test]))` pour le jeu de donnée original.

<u>Jeu de donnée original</u>:

- Nombre d'observations d'entraînement: `r length(origin_train[origin_train %in% Original_test])` (`r round((length(origin_train[origin_train %in% Original_test])/(length(origin_train[origin_train %in% Original_test])+length(origin_test[origin_test %in% Original_test])))*100,2)`%).

- Nombre d'observations de test: `r length(origin_test[origin_test %in% Original_test])` (`r round(length(origin_test[origin_test %in% Original_test])*100/(length(origin_test[origin_test %in% Original_test])+length(origin_train[origin_train %in% Original_test])),2)`%).

<u>Jeu de donnée équilibré</u>:

- Nombre d'observations d'entraînement: `r nrow(data_train)` (`r round((nrow(data_train)/(nrow(data_train)+nrow(data_test)))*100,2)`%).

- Nombre d'observations de test: `r nrow(data_test)` (`r round((nrow(data_test)/(nrow(data_train)+nrow(data_test)))*100,2)`%).

# Test n et p

Pour les modèle random forest et SVM les paramètres par défaut on été conservé pour les tests n et p. Du fait de la sensibilité de XGBoost au choix des paramètres et de l'absence de paramètre par défaut pour les réseaux de neurones un tuning basique a été réalisé pour choisir les paramètres de ces 2 modèles.

Pour permettre une meilleure compareront entre le jeu de donnée équilibré et le jeu de donnée originale les tests, on été effectué à la fois sur le jeu équilibré en utilisent toutes les 87 classes. Et aussi en conservent uniquement le 69 classes testé dans le jeu de donnée non équilibré.

## Donnée original 69 classes

```{r}
rf_Oldres = readRDS("OldDataOutput/randomForest_rf/randomForest_rf_res.rds")
svm_Oldres = readRDS("OldDataOutput/e1071_svm/e1071_svm_res.rds")
xgb_Oldres = readRDS("OldDataOutput/xgboost_xgboost/xgboost_xgboost_res.rds")
nn_Oldres = convert_py_res(read_json("OldDataOutput/nn_600_300_np/nn_600_300_np_res.json"))
```

:::{#fig-np_oigin}

```{r, fig.width=15, fig.height=16}
ggarrange(plot_res_np(rf_Oldres, "acc_train"), 
          plot_res_np(rf_Oldres, "acc_test"),
          plot_res_np(rf_Oldres, "time"),
          plot_res_np(svm_Oldres, "acc_train"), 
          plot_res_np(svm_Oldres, "acc_test"),
          plot_res_np(svm_Oldres, "time"),
          plot_res_np(xgb_Oldres, "acc_train"), 
          plot_res_np(xgb_Oldres, "acc_test"),
          plot_res_np(xgb_Oldres, "time"),
          plot_res_np(nn_Oldres, "acc_train"), 
          plot_res_np(nn_Oldres, "acc_test"),
          plot_res_np(nn_Oldres, "time"),
          labels = c("RF", "", "","SVM", "", "","XGB", "", "","NN"),
          ncol = 3, nrow = 4)
```

Résultat des temps de calcul et de l'accuracy pour les jeux d'entraînement et de test en fonction du nombre d'observations et du nombre de variables pour chaque modèle pour les données originales non équilibré (69) classes.

:::

```{r}
temp = data.frame("Random forest" = c(rf_Oldres$newas10000$acc_test[10], unpack_time(rf_Oldres, "newas10000", "elapsed")[10]),
                  "SVM" = c(svm_Oldres$newas10000$acc_test[10], unpack_time(svm_Oldres, "newas10000", "elapsed")[10]),
                  "XgBoost" = c(xgb_Oldres$newas10000$acc_test[10], unpack_time(xgb_Oldres, "newas10000", "elapsed")[10]),
                  "NN" = c(nn_Oldres$newas10000$acc_test[10], unpack_time(nn_Oldres, "newas10000", "elapsed")[10])) 
rownames(temp) = c("Test accuracy", "Temp de calcul en seconds")
kbl(round(temp, 2), align = "c", caption = "Table 1: Résultats des modèles avec les données originales pour n=1511 et p=10000 avec 69 classes.")
```

## Donnée équilibrée 87 classes

```{r}
rf_res = readRDS("Output/randomForest_rf/randomForest_rf_res.rds")
xgb_res = readRDS("Output/xgboost_xgboost/xgboost_xgboost_res.rds")
svm_res = readRDS("Output/e1071_svm/e1071_svm_res.rds")
nn_res = convert_py_res(read_json("Python/Output/nn_600_300_np/nn_600_300_np_res.json"))
```


:::{#fig-np_balanced87}

```{r, fig.width=15, fig.height=16}
ggarrange(plot_res_np(rf_res, "acc_train"), 
          plot_res_np(rf_res, "acc_test"),
          plot_res_np(rf_res, "time"),
          plot_res_np(svm_res, "acc_train"), 
          plot_res_np(svm_res, "acc_test"),
          plot_res_np(svm_res, "time"),
          plot_res_np(xgb_res, "acc_train"), 
          plot_res_np(xgb_res, "acc_test"),
          plot_res_np(xgb_res, "time"),
          plot_res_np(nn_res, "acc_train"), 
          plot_res_np(nn_res, "acc_test"),
          plot_res_np(nn_res, "time"),
          labels = c("RF", "", "","SVM", "", "","XGB", "", "","NN"),
          ncol = 3, nrow = 4)
```

Résultat des temps de calcul et de l'accuracy pour les jeux d'entraînement et de test en fonction du nombre d'observations et du nombre de variables pour chaque modèle pour les données équilibrée, 87 classes.

:::

```{r}
temp = data.frame("Random forest" = c(rf_res$newas10000$acc_test[10], unpack_time(rf_res, "newas10000", "elapsed")[10]),
                  "SVM" = c(svm_res$newas10000$acc_test[10], unpack_time(svm_res, "newas10000", "elapsed")[10]),
                  "XgBoost" = c(xgb_res$newas10000$acc_test[10], unpack_time(xgb_res, "newas10000", "elapsed")[10]),
                  "NN" = c(nn_res$newas10000$acc_test[10], unpack_time(nn_res, "newas10000", "elapsed")[10])) 
rownames(temp) = c("Test accuracy", "Temp de calcul en seconds")
kbl(round(temp, 2), align = "c", caption = "Table 2: Résultats des modèles avec donnée équilibrée 87 classes, pour n=1511 et p=10000")
```


## Donnée équilibrée 69 classes

```{r}
rf_Orires = readRDS("ReducedOutput/randomForest_rf/randomForest_rf_res.rds")
svm_Orires = readRDS("ReducedOutput/e1071_svm/e1071_svm_res.rds")
xgb_Orires = readRDS("ReducedOutput/xgboost_xgboost/xgboost_xgboost_res.rds")
nn_Orires = convert_py_res(read_json("ReducedOutput/nn_600_300_np/nn_600_300_np_res.json"))
```

:::{#fig-np_balanced69}

```{r, fig.width=15, fig.height=16}
ggarrange(plot_res_np(rf_Orires, "acc_train"), 
          plot_res_np(rf_Orires, "acc_test"),
          plot_res_np(rf_Orires, "time"),
          plot_res_np(svm_Orires, "acc_train"), 
          plot_res_np(svm_Orires, "acc_test"),
          plot_res_np(svm_Orires, "time"),
          plot_res_np(xgb_Orires, "acc_train"), 
          plot_res_np(xgb_Orires, "acc_test"),
          plot_res_np(xgb_Orires, "time"),
          plot_res_np(nn_Orires, "acc_train"), 
          plot_res_np(nn_Orires, "acc_test"),
          plot_res_np(nn_Orires, "time"),
          labels = c("RF", "", "","SVM", "", "","XGB", "", "","NN"),
          ncol = 3, nrow = 4)
```

Résultat des temps de calcul et de l'accuracy pour les jeux d'entraînement et de test en fonction du nombre d'observations et du nombre de variables pour chaque modèle pour les données équilibrée, 69 classes.

:::

```{r}
temp = data.frame("Random forest" = c(rf_Orires$newas10000$acc_test[10], unpack_time(rf_Orires, "newas10000", "elapsed")[10]),
                  "SVM" = c(svm_Orires$newas10000$acc_test[10], unpack_time(svm_Orires, "newas10000", "elapsed")[10]),
                  "XgBoost" = c(xgb_Orires$newas10000$acc_test[10], unpack_time(xgb_Orires, "newas10000", "elapsed")[10]),
                  "NN" = c(nn_Orires$newas10000$acc_test[10], unpack_time(nn_Orires, "newas10000", "elapsed")[10])) 
rownames(temp) = c("Test accuracy", "Temp de calcul en seconds")
kbl(round(temp, 2), align = "c", caption = "Table 3: Résultats des modèles avec donnée équilibrée pour n=1511 et p=10000 avec 87 classes.")
```

## Comparaison entre jeu de données

```{r}
temp = data.frame("Train Accuracy" = c(rf_Oldres$newas10000$acc_train[10], 
                                       rf_res$newas10000$acc_train[10], 
                                       rf_Orires$newas10000$acc_train[10]),
                  "Test Accuracy" = c(rf_Oldres$newas10000$acc_test[10], 
                                      rf_res$newas10000$acc_test[10], 
                                      rf_Orires$newas10000$acc_test[10]),
                  "Time in seconds" = c(rf_Oldres$newas10000$time$n1600[3], 
                                      rf_res$newas10000$time$n1600[3], 
                                      rf_Orires$newas10000$time$n1600[3]))
rownames(temp) = c("Original data (69 classes)", "Balanced data (87 classes)", "Balanced data (69 classes)")
kbl(temp[order(temp$Test.Accuracy,decreasing = TRUE),], align = "c", caption = "Table 4: Comparaison résultats des modèles random forest pour n=1511 et p=10000")
```


```{r}
temp = data.frame("Train Accuracy" = c(svm_Oldres$newas10000$acc_train[10], 
                                       svm_res$newas10000$acc_train[10], 
                                       svm_Orires$newas10000$acc_train[10]),
                  "Test Accuracy" = c(svm_Oldres$newas10000$acc_test[10], 
                                      svm_res$newas10000$acc_test[10], 
                                      svm_Orires$newas10000$acc_test[10]),
                  "Time in seconds" = c(svm_Oldres$newas10000$time$n1600[3], 
                                        svm_res$newas10000$time$n1600[3], 
                                        svm_Orires$newas10000$time$n1600[3]))
rownames(temp) = c("Original data (69 classes)", "Balanced data (87 classes)", "Balanced data (69 classes)")
kbl(temp[order(temp$Test.Accuracy,decreasing = TRUE),], align = "c", caption = "Table 5: Comparaison résultats des modèles SVM pour n=1511 et p=10000")
```

```{r}
temp = data.frame("Train Accuracy" = c(xgb_Oldres$newas10000$acc_train[10], 
                                       xgb_res$newas10000$acc_train[10], 
                                       xgb_Orires$newas10000$acc_train[10]),
                  "Test Accuracy" = c(xgb_Oldres$newas10000$acc_test[10], 
                                      xgb_res$newas10000$acc_test[10], 
                                      xgb_Orires$newas10000$acc_test[10]),
                  "Time in seconds" = c(xgb_Oldres$newas10000$time$n1600[3], 
                                        xgb_res$newas10000$time$n1600[3], 
                                        xgb_Orires$newas10000$time$n1600[3]))
rownames(temp) = c("Original data (69 classes)", "Balanced data (87 classes)", "Balanced data (69 classes)")
kbl(temp[order(temp$Test.Accuracy,decreasing = TRUE),], align = "c", caption = "Table 6: Comparaison résultats des modèles XGBoost pour n=1511 et p=10000")
```

```{r}
temp = data.frame("Train Accuracy" = c(nn_Oldres$newas10000$acc_train[10], 
                                       nn_res$newas10000$acc_train[10], 
                                       nn_Orires$newas10000$acc_train[10]),
                  "Test Accuracy" = c(nn_Oldres$newas10000$acc_test[10], 
                                      nn_res$newas10000$acc_test[10], 
                                      nn_Orires$newas10000$acc_test[10]),
                  "Time in seconds" = c(nn_Oldres$newas10000$time[10], 
                                        nn_res$newas10000$time[10], 
                                        nn_Orires$newas10000$time[10]))
rownames(temp) = c("Original data (69 classes)", "Balanced data (87 classes)", "Balanced data (69 classes)")
kbl(temp[order(temp$Test.Accuracy,decreasing = TRUE),], align = "c", caption = "Table 7: Comparaison résultats des modèles réseaux de neurones pour n=1511 et p=10000")
```

On remarquera que pour tous les modèles le temps de computation semble être quasiment linéaire. Avec la pente de la droite étant influencer par le nombre de variables considéré.

De façon générale l'ajout du nombre d'observations apport un gain d'accuracy décroisant avec le nombre d'observation. La vitesse de cette décroissance initiale d'accuracy en fonction du nombre d'individus augmente fortement avec le nombre de variables inclue dans les modèles. Cette sensibilité aux nombres de variables varie en fonction des modèles.

## Résultats généraux détaillés


### Random Forest

- Language: R
- Package: randomForest

Le random forest est de loin le modèle qui prend le plus de temps a calculer. Il est probable que cela soit dû au fait que le modèle calcule la pureté des nœuds pour les "split" de toutes les variables restent. Ceci est exacerbé par le fait que pour des variables continues il doit calculer le "split" pour chaque valeur existante de la variable présente dans le jeu de données.

Le random forest semble être le modèle qui a sont accuracy le mois affecté par un nombre de variables réduits, mais de ce fait plus sensible au nombre d'observations.

On remarquera que le random forest classifie toujours parfaitement notre jeu de donnée d'entraînement. Il est probable que ceci soit impacté par le découpage en nœud précis fait avec les données continues.

#### Liste des paramètres utilisés

- **mtry** = sqrt(p):

- **ntree:** 500

#### Classification par classe random forest

Les classes sur l'axe des abscisses sont ordonnées en fonction de leur prévalence dans le jeu de donnée entier de manière descendante.

:::{#fig-classif_rf}

```{r, fig.width=15, fig.height=7}
plot_class(rf_Oldres$newas10000$results$pred_labs, rf_Oldres$newas10000$results$test_labs)
```

Classification par classe du modèle random forest pour n = 1511 et p = 10000.

:::

### SVM

- Language: R
- Package: e1071

Le svm semble aussi être sensible au nombre de variables utilisées. Cependant, il converge plus vite que le réseau de neurones.

Nous sommes en train de comprendre les mathématiques utilisées pas le SVM pour pouvoirs expliquer sa performance.

La baisse de l'accuracy pour les modèles avec un petit nombre de variables peut s'expliquer par la difficulté de tracer un hyperplan pour bien séparer les données plus le nombre d'observations est élevé. Mais plus le nombre de variables plus le nombre de dimensions augmente permettent de pouvoirs séparer les données.


#### Liste des paramètres utilisés

- **type** = "C-classification"

- **kernel** = "linear"

- **cost** = 1

- **epsilon** = 0.1

- **shrinking** = TRUE

- **probability** = TRUE

#### Classification par classe SVM

Les classes sur l'axe des abscisses sont ordonnées en fonction de leur prévalence dans le jeu de donnée entier de manière descendante.

:::{#fig-classif_svm}

```{r, fig.width=15, fig.height=7}
plot_class(svm_Oldres$newas10000$results$pred_labs, svm_Oldres$newas10000$results$test_labs)
```

Classification par classe du modèle SVM pour n = 1511 et p = 10000.

:::

### XGBoost

- Language: R
- Package: xgboost
 
On remarque que l'accuracy et particulièrement mauvaises pour les petites valeurs de n. Cela est dû au fait que similairement au réseau de neurones XGBoost utilise la décente de gradient. Or contrairement au modèle nn présenté ici la vitesse de la détente de gradient et fix. Cela présente un problème, car plus le nombre d'observation et petites plus la vitesse de la décente de gradient doit être élevé. Ceci se voit lors de l'entraînement du modèle ou la fonction de perte ne diminué pas pour des modèles avec un nombre d'observations faible.

Nous sommes toujours en train de nous renseigner sur le fonctionnement de XGBoost car ce modèle est particulièrement compliqué.

#### Liste des paramètres utilisés

- **nrounds** = 30

- **booster** = "gbtree"

- **eta** = 0.15

- **max_depth** = 13

- **subsample** = 1: Proportion of datapoints to use for training

- **lambda** = 3: L2 regularization

- **alpha** = 0: L1 regularization

- **objective** = "multi:softprob"

- **gamma** = 0

- **min_child_weight** = 0.1

- **colsample_bytree** = 0.1


#### Classification par classe XGBoost

Les classes sur l'axe des abscisses sont ordonnées en fonction de leur prévalence dans le jeu de donnée entier de manière descendante.

:::{#fig-classif_xgb}

```{r, fig.width=15, fig.height=7}
plot_class(xgb_Oldres$newas10000$results$pred_labs, xgb_Oldres$newas10000$results$test_labs)
```

Classification par classe du modèle XGBoost pour n = 1511 et p = 10000.

:::

### Réseau de neurones

- Language: python
- Package: Tensorflow, keras

On peut remarquer dans la figure 5 que le réseau de neurones est particulièrement sensible au nombre de variables inclue dans le modèle. Cette tendance se retrouve aussi dans l'accuracy du jeux d'entraînement.

#### Structure du réseau

Les réseaux de neurones n'ayant pas de paramètre par défaut, nous avons choisi la structure de réseaux en référence à la littérature sur la classification des tumeurs du système nerveux central en fonction de la méthylation de l'ADN.
Nous avons donc choisi un réseau en 2 couches (600,300) étant une structure proche des autres travaux sur la classification des cancers en fonction de la méthylation de l'ADN.

Cette petite structure est souvent due au fait que dans la littérature des couches d'extractions de features sont présente en amont de la couche connecté. Or, la sélection de variables faite en amont peuvent être considérée comme une sorte d'extraction de features. De ce fait, cette petite structure semble justifier.

- **learning rate initial:** `lr = 0.001`

- **learning rate sheduler:** `lr * math.exp(-0.1)` 

- **epochs:** 60

- **Couche 1:** taille = 600, fonction d'activation = "linear"

- **Couche 2:** taille = 300, fonction d'activation = "linear"


#### Classification par classe réseau de neurones

Les classes sur l'axe des abscisses sont ordonnées en fonction de leur prévalence dans le jeu de donnée entier de manière descendante.

:::{#fig-classif_nn}

```{r, fig.width=15, fig.height=7}
plot_class(unlist(nn_Oldres$newas10000$results$pred_labs), unlist(nn_Oldres$newas10000$results$test_labs))
```

Classification par classe du modèle nn (600,300) pour n = 1511 et p = 10000.

:::

# Optimisation

```{r}
svm_Oldopti = res_param_df(readRDS("OldDataOutput/e1071_svm_param/e1071_svm_param_res.rds"), order = "acc_test")
svm_cor_plot = svm_Oldopti
svm_cor_plot$kernel = as.numeric(factor(svm_Oldopti$kernel, labels = c(1,2,3)))
svm_cor_plot = apply(svm_cor_plot, 2, as.numeric)
xgb_Oldopti = res_param_df(readRDS("OldDataOutput/xgboost_xgboost_param/xgboost_xgboost_param_res.rds"), order = "acc_test")
```

**Tous les résultats suivent sont ceux du jeux de donnée original a 69 classes.**

## Choix des paramètres

Dans un premier temps, nous avons testé l'impacte des paramètres de façon univarié pour déterminer la plage de valeurs a testé pour chaque paramètre. Une fois celle-ci déterminée, une recherche sur l'ensemble des combinaisons des paramètres a été effectuée.

Cette recherche a ensuite été affinée de manière itérative pour essayer d'affiner le choix des paramètres.

### SVM

Gamma not tested as has negligeble impact on accuracy

#### List des paramètres testés
```{r}
#apply(svm_Oldopti[svm_Oldopti$kernel == "linear",2:5], 2, unique)
print("Kernel: linear, polynomial, radial")
apply(svm_Oldopti[svm_Oldopti$kernel == "polynomial",-c(1,6:8)], 2, unique)
#apply(svm_Oldopti[svm_Oldopti$kernel == "radial",1:2], 2, unique)
```

:::{#fig-cor_svm_all}

```{r}
corrplot::corrplot(cor(svm_cor_plot)[-c(1:5),-c(3:5)], 
                   method = "square", 
                   number.digits = 2, 
                   p.mat = cor(svm_cor_plot)[-c(1:5),-c(3:5)], 
                   sig.level=-1, 
                   tl.cex = 1,
                   insig = "p-value")
```

Corrélation entre les paramètres de SVM, l'accuracy et le temps de calcul pour tout les kernel.

:::

:::{#fig-cor_svm_poly}

```{r, warning=FALSE}
corrplot::corrplot(cor(svm_cor_plot[svm_cor_plot[,1] == 2,])[-c(1:5),-c(1,3)], 
                   method = "square", 
                   number.digits = 2, 
                   p.mat = cor(svm_cor_plot[svm_cor_plot[,1] == 2,])[-c(1:5),-c(1,3)], 
                   sig.level=-1, 
                   tl.cex = 1,
                   insig = "p-value")
```

Corrélation entre les paramètres de SVM, l'accuracy et le temps de calcul pour le kernel polynomial.

:::

Les coef0 = 0 ont la moins bonne performance et prennent le plus de temps a calculé. D'où la forte corrélation négative entre le temps et l'accuracy.


### XGBoost

#### List des paramètres testés

```{r}
sapply(apply(xgb_Oldopti[,-c(8:12)], 2, unique), sort)
```

:::{#fig-cor_xgb}

```{r}
corrplot::corrplot(cor(xgb_Oldopti)[8:10,], 
                   method = "square", 
                   number.digits = 2, 
                   p.mat = cor(xgb_Oldopti)[8:10,], 
                   sig.level=-1, 
                   tl.cex = 1,
                   insig = "p-value")
```

Corrélation entre les paramètres de XGBoost, l'accuracy et le temps de calcul.

:::


:::{#fig-pairs_xgb}

```{r, fig.width=15, fig.height=10, message=FALSE, warning=FALSE}
GGally:::ggpairs(xgb_Oldopti[,-c(1,8,10:12)])
```

Relation entre les paramètres et l'accuracy des données test.

:::

## Validation croisée

### XGBoost

```{r}
xgb_Oldcv = cv_eval(readRDS("OldDataOutput/xgboost_CV/xgboost_CV_res.rds"))
svm_Oldcv = cv_eval(readRDS("OldDataOutput/svm_cv/svm_CV_res.rds"))
```

```{r}
kbl(xgb_Oldcv[1:5,1:10], format = "html", row.names = FALSE, digits = 2,
    caption = "Table 8: Les 5 meilleur modèles avec validation croisé a 3 folds")
```


```{r, eval=FALSE}
temp_original_train = readRDS(paste0("OldData/df_newas10000_train.rds"))
temp_original_test = readRDS(paste0("OldData/df_newas10000_test.rds"))

if(length(unique(temp_original_train$meth_class)) != length(unique(temp_original_test$meth_class))){
    shared_classes = intersect(temp_original_train$meth_class, temp_original_test$meth_class)

    temp_original_train = temp_original_train[temp_original_train$meth_class %in% shared_classes,]
    temp_original_test = temp_original_test[temp_original_test$meth_class %in% shared_classes,]
  }

xgb_train = xgb.DMatrix(as.matrix(temp_original_train[,-1]), 
                        label = as.integer(factor(temp_original_train[,1]))-1)
xgb_test = xgb.DMatrix(as.matrix(temp_original_test[,-1]), 
                       label = as.integer(factor(temp_original_test[,1]))-1)

time = system.time({model_xgb_best = xgboost(data = xgb_train,
                                             # max number of boosting iterations
                                             nrounds = 30,
                                             verbose = 0,
                                             params = list(booster = "gbtree",
                                                           # learning rate
                                                           eta = xgb_Oldcv[1,1],
                                                           max_depth = xgb_Oldcv[1,7],
                                                           # L2 regularization
                                                           lambda = xgb_Oldcv[1,3],
                                                           # L1 regularization
                                                           alpha = xgb_Oldcv[1,4],
                                                           # minimum loss for next leaf
                                                           gamma = xgb_Oldcv[1,2],
                                                           min_child_weight = xgb_Oldcv[1,5],
                                                           colsample_bytree = xgb_Oldcv[1,6],
                                                           # Proportion of datapoints to use for training
                                                           subsample = 1,
                                                           # The loss function
                                                           objective = "multi:softprob",
                                                           num_class = length(unique(temp_original_test$meth_class))))})["elapsed"]

pred_train = predict(model_xgb_best,xgb_train,reshape=T)
pred_train = as.data.frame(pred_train)
colnames(pred_train) = levels(factor(temp_original_train[,1]))
pred_train = apply(pred_train, 1, function(x){names(which.max(x))})
pred_train = as.character(pred_train) == as.character(temp_original_train$meth_class)

pred_test = predict(model_xgb_best,xgb_test,reshape=T)
pred_test = as.data.frame(pred_test)
colnames(pred_test) = levels(factor(temp_original_test[,1]))
pred_test_lab = apply(pred_test, 1, function(x){names(which.max(x))})
pred_test = as.character(pred_test_lab) == as.character(temp_original_test$meth_class)

model_xgb_best_res = c("acc_train" = sum(pred_train)*100/length(pred_train),
                       "acc_test" = sum(pred_test)*100/length(pred_test),
                       "time" = time)

saveRDS(pred_test_lab, "OldDataOutput/xgboost_CV/xgb_best_model_pred.rds", compress = TRUE)
saveRDS(model_xgb_best, "OldDataOutput/xgboost_CV/best_model_xgb.rds", compress = TRUE)
saveRDS(model_xgb_best_res, "OldDataOutput/xgboost_CV/best_model_xgb_res.rds", compress = TRUE)
```

```{r}
temp = round(readRDS("OldDataOutput/xgboost_CV/best_model_xgb_res.rds"), 2)
kbl(t(c(xgb_Oldcv[1,1:7],temp)), format = "html", row.names = FALSE,
    caption = "Table 9: Le meilleur modèle XGBoost appliqué au jeu de données entraînement et test.")
```


### SVM

```{r}
temp = svm_Oldcv
temp[,2:ncol(temp)] = round(apply(svm_Oldcv[,2:ncol(temp)], 2, as.numeric),2)

kbl(temp[1:8,], format = "html", row.names = FALSE,
    caption = "Table 10: Les 8 meilleur modèles avec validation croisé a 3 folds")
```

Malgré le faite que certains modèles à kernel polynomial ont une meilleur accuracy d'environ 0.03% que les modèles à kernel linéaire, il obtienne la même accuracy (83.65651%) sur les jeux d'entraînement et de test complet.

```{r, eval=FALSE}
temp_original_train = readRDS(paste0("OldData/df_newas10000_train.rds"))
temp_original_test = readRDS(paste0("OldData/df_newas10000_test.rds"))

if(length(unique(temp_original_train$meth_class)) != length(unique(temp_original_test$meth_class))){
    shared_classes = intersect(temp_original_train$meth_class, temp_original_test$meth_class)

    temp_original_train = temp_original_train[temp_original_train$meth_class %in% shared_classes,]
    temp_original_test = temp_original_test[temp_original_test$meth_class %in% shared_classes,]
}

temp_original_train$meth_class = factor(temp_original_train$meth_class)
temp_original_test$meth_class = factor(temp_original_test$meth_class)

time = system.time({model_svm_best = svm(x = temp_original_train[,-1],
                                         y = temp_original_train[,1],
                                         type = "C-classification",
                                         kernel = svm_Oldopti[1,1],
                                         cost = as.numeric(svm_Oldcv[1,2]),
                                         #gamma = as.numeric(svm_Oldcv[1,3]),
                                         #coef0 = as.numeric(svm_Oldcv[1,4]),
                                         #degree = as.numeric(svm_Oldcv[1,5]),
                                         class.weights = table(temp_original_train[,1]),
                                         probability = TRUE)})["elapsed"]

pred_train = predict(model_svm_best, newdata = temp_original_train[,-1])

pred_test_lab = predict(model_svm_best, newdata = temp_original_test[,-1])


# Putting all results in result vectors
pred_train = as.character(pred_train) == as.character(temp_original_train$meth_class)
pred_test = as.character(pred_test_lab) == as.character(temp_original_test$meth_class)
model_svm_best_res = c("acc_train" = sum(pred_train)*100/length(pred_train),
                       "acc_test" = sum(pred_test)*100/length(pred_test),
                       "time" = time)


saveRDS(model_svm_best_res, "OldDataOutput/svm_cv/best_model_svm_res.rds", compress = TRUE)
```

```{r}
temp = round(readRDS("OldDataOutput/svm_cv/best_model_svm_res.rds"), 2)
kbl(t(c(svm_Oldcv[1,1:5],temp)), format = "html", row.names = FALSE,
    caption = "Table 11: Le meilleur modèle SVM appliqué au jeu de données entraînement et test.")
```



# Annexes

## Test np modèles optimaux

:::{#fig-np_opti}

```{r, fig.width=15, fig.height=16}
xgbOld_np_opti = readRDS("OldDataOutput/xgboost_xgboost_np_opti/xgboost_xgboost_np_opti_res.rds")
svmOld_np_opti = readRDS("OldDataOutput/e1071_svm_np_opti/e1071_svm_np_opti_res.rds")
ggarrange(plot_res_np(rf_Oldres, "acc_train"), 
          plot_res_np(rf_Oldres, "acc_test"),
          plot_res_np(rf_Oldres, "time"),
          plot_res_np(svmOld_np_opti, "acc_train"), 
          plot_res_np(svmOld_np_opti, "acc_test"),
          plot_res_np(svmOld_np_opti, "time"),
          plot_res_np(xgbOld_np_opti, "acc_train"), 
          plot_res_np(xgbOld_np_opti, "acc_test"),
          plot_res_np(xgbOld_np_opti, "time"),
          plot_res_np(nn_Oldres, "acc_train"), 
          plot_res_np(nn_Oldres, "acc_test"),
          plot_res_np(nn_Oldres, "time"),
          labels = c("RF", "", "","SVM", "", "","XGB", "", "","NN"),
          ncol = 3, nrow = 4)
```

Résultat des tests n et p avec les modèles SVM et XGBoost optimal présenté dans les tables 9 et 11.

:::

## Spécification de l'ordinateur utilisé

- Processor: AMD Ryzen 7 5800H with Radeon Graphics, 3201 Mhz, 8 Core(s), 16 Logical Processor(s)

- RAM: 16G

- OS: Microsoft Windows 11 Home

